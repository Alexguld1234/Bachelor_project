data.py
-----
import argparse
from pathlib import Path
import random
import numpy as np
import pandas as pd
import torch
from PIL import Image, ImageFile
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset, random_split


ImageFile.LOAD_TRUNCATED_IMAGES = True
csv_file="local/Final_AP_url_label_50000.csv"


def set_seed(seed: int = 42) -> None:
    """Ensure reproducible results across Python, NumPy and PyTorch."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


class ChestXrayDataset(Dataset):
    def __init__(self,
                mode: str = "train",
                split: tuple = (0.8, 0.1, 0.1),
                setup: str = "local",
                img_size: tuple = (224, 224),
                num_samples: int = None,
                csv_file: Path = Path("local/Final_AP_url_label_50000.csv")):   # NEW):
        set_seed(42)
        self.data = pd.read_csv(csv_file)

        if num_samples is not None:
            self.data = self.data.sample(n=num_samples, random_state=42).reset_index(drop=True)
        
        self.mode = mode
        self.transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize(img_size),
            transforms.ToTensor(),
        ])

        if setup == "local":
            self.data["txt_path"] = self.data["local_txt_urls"].apply(Path)
            self.data["jpg_path"] = self.data["local_urls"].apply(Path)
        else:
            self.data["txt_path"] = self.data["hpc_txt_urls"].apply(Path)
            self.data["jpg_path"] = self.data["hpc_urls"].apply(Path)

        dataset_size = len(self.data)
        train_size = int(split[0] * dataset_size)
        val_size = int(split[1] * dataset_size)
        test_size = dataset_size - train_size - val_size
        
        self.train_data, self.val_data, self.test_data = random_split(
            self.data.to_dict(orient="records"), [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)
        )

        if self.mode == "train":
            self.current_data = self.train_data
        elif self.mode == "val":
            self.current_data = self.val_data
        elif self.mode == "test":
            self.current_data = self.test_data
        else:
            raise ValueError("Mode must be 'train', 'val', or 'test'.")
        

    def __len__(self):
        return len(self.current_data)
    
    def __getitem__(self, idx):
        sample = self.current_data[idx]


        # Load image
        img_path = sample["jpg_path"]
        image = Image.open(img_path)
        image = self.transform(image)

        txt_path = sample["txt_path"]
        try:
            with open(txt_path, "r") as f:
                text_report = f.read()
        except FileNotFoundError:
            print(f"File not found: {txt_path}")
            text_report = ""

        label = torch.tensor(sample.get("pneumonia_label", 3), dtype=torch.long)

        return image, text_report, label
    

def get_dataloader(mode: str = "train",
                   batch_size: int = 32,
                   shuffle: bool = True,
                   num_samples: int = None,
                   img_size: tuple = (224, 224),
                   setup: str = "local",                                 # NEW
                   csv_file: Path = Path("local/Final_AP_url_label_50000.csv") # NEW
                   ):
    dataset = ChestXrayDataset(mode=mode,
                               num_samples=num_samples,
                               img_size=img_size,
                               setup=setup,
                               csv_file=csv_file)                        # pass them on
    return DataLoader(dataset,
                      batch_size=batch_size,
                      shuffle=shuffle)
def main():
    parser = argparse.ArgumentParser(description="Chest X-ray Dataset Loader")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "val", "test"], help="Mode of the dataset")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for DataLoader")
    parser.add_argument("--num_samples", type=int, default=None, help="Number of samples to load from the dataset")
    parser.add_argument("--shuffle", action='store_true', help="Shuffle the dataset")
    parser.add_argument("--no_shuffle", dest="shuffle", action='store_false', help="Disable shuffling")
    parser.set_defaults(shuffle=True)

    parser.add_argument("--setup", choices=["local", "hpc"], default="local")
    parser.add_argument("--csv_file", type=str, default="local/Final_AP_url_label_50000.csv")
    parser.add_argument("--img_size", type=int, default=224, help="Square side length")
    args = parser.parse_args()

    dataloader = get_dataloader(
        mode=args.mode,
        batch_size=args.batch_size,
        shuffle=args.shuffle,
        num_samples=args.num_samples,
        img_size=(args.img_size, args.img_size),
        setup=args.setup,
        csv_file=Path(args.csv_file),
)
    
    for images, text_reports, labels in dataloader:
        print(f"Images batch shape: {images.size()}")
        print(f"Text reports batch size: {len(text_reports)}")
        print(f"Labels batch shape: {labels.size()}")
        break

if __name__ == "__main__":
    main()
-----
radtex_model.py
-----
# model.py
import torch
import torch.nn as nn
import torchvision.models as models
from transformers import AutoModelForCausalLM, GPT2LMHeadModel, GPT2Config
import torchxrayvision as xrv

# ---------------- Encoder Options ----------------

class ResNet50Backbone(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)
        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.conv1.weight = nn.Parameter(torch.mean(resnet50.conv1.weight, dim=1, keepdim=True))
        self.feature_extractor = nn.Sequential(*list(resnet50.children())[1:-2])
        self.flatten = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        x = self.conv1(x)
        x = self.feature_extractor(x)
        x = self.flatten(x)
        return torch.flatten(x, 1)

class DenseNet121Backbone(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        self.model = xrv.models.DenseNet(weights="all" if pretrained else None)
        self.model.op_threshs = None  # Turn off built-in thresholds

    def forward(self, x):
        features = self.model.features(x)
        features = torch.nn.functional.relu(features, inplace=True)
        features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))
        return features.view(features.size(0), -1)

class ScratchCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1,1))
        )
    
    def forward(self, x):
        x = self.conv(x)
        return torch.flatten(x, 1)

# ---------------- Decoder Options ----------------

class ScratchTransformer(nn.Module):
    def __init__(self, vocab_size, hidden_dim=256, num_layers=2, num_heads=4):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, vocab_size)

    def forward(self, input_ids, encoder_hidden_states=None):
        x = self.embedding(input_ids)
        if encoder_hidden_states is not None:
            if encoder_hidden_states.dim() == 2:  # [B, D]
                encoder_hidden_states = encoder_hidden_states.unsqueeze(1)  # [B, 1, D]
            x = x + encoder_hidden_states  # broadcast to [B, T, D]
  # add features as bias
        x = self.transformer(x)
        logits = self.fc_out(x)
        return logits
    def generate(self, input_ids, encoder_hidden_states=None, max_length=128, **kwargs):
        """
        Greedy decoding for the scratch transformer.
        input_ids: [batch_size, seq_len] (prompt tokens)
        encoder_hidden_states: [batch_size, hidden_dim]
        """
        generated = input_ids.clone()
        batch_size = input_ids.size(0)
        device = input_ids.device

        for _ in range(max_length - input_ids.size(1)):
            logits = self.forward(generated, encoder_hidden_states=encoder_hidden_states)
            next_token_logits = logits[:, -1, :]  # get logits for last token
            next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # greedy pick
            generated = torch.cat((generated, next_token), dim=1)

        return generated


# ---------------- Full Model ----------------

class RadTexModel(nn.Module):
    def __init__(self, encoder_name, decoder_name, vocab_size, num_classes=4, pretrained_backbone=True):
        super().__init__()

        # Select Encoder
        if encoder_name == "resnet":
            self.visual_backbone = ResNet50Backbone(pretrained=pretrained_backbone)
            visual_feature_dim = 2048
        elif encoder_name == "densenet":
            self.visual_backbone = DenseNet121Backbone(pretrained=pretrained_backbone)
            visual_feature_dim = 1024
        elif encoder_name == "scratch_encoder":
            self.visual_backbone = ScratchCNN()
            visual_feature_dim = 64
        else:
            raise ValueError(f"Unknown encoder: {encoder_name}")

        # Select Decoder
        if decoder_name == "gpt2":
            config = GPT2Config.from_pretrained("gpt2")
            config.add_cross_attention = True
            config.vocab_size = vocab_size
            config.pad_token_id = config.eos_token_id
            config.bos_token_id = config.bos_token_id or config.eos_token_id
            self.text_decoder = GPT2LMHeadModel(config)
            self.text_decoder.resize_token_embeddings(vocab_size)
        elif decoder_name == "biogpt":
            self.text_decoder = AutoModelForCausalLM.from_pretrained("microsoft/biogpt")
        # Only resize if vocab size doesn't match
            if self.text_decoder.config.vocab_size != vocab_size:
                self.text_decoder.resize_token_embeddings(vocab_size)

        elif decoder_name == "scratch_decoder":
            self.text_decoder = ScratchTransformer(vocab_size)
        else:
            raise ValueError(f"Unknown decoder: {decoder_name}")
        if decoder_name == "scratch_decoder":
            decoder_input_dim = 256  # Match hidden_dim from ScratchTransformer
        else:
            decoder_input_dim = 768

        self.feature_projection = nn.Linear(visual_feature_dim, decoder_input_dim)

        self.classifier = nn.Sequential(
            nn.Linear(visual_feature_dim, 512),
            nn.ReLU(),
            nn.Linear(512, num_classes),
            nn.Sigmoid()
        )

    def forward(self, images, text_inputs=None, generate=False, generation_args=None):
        visual_features = self.visual_backbone(images)
        projected_features = self.feature_projection(visual_features)
        classification_output = self.classifier(visual_features)

        if generate and text_inputs is not None:
            encoder_hidden_states = projected_features.unsqueeze(1)
            attention_mask = torch.ones_like(text_inputs)

            if hasattr(self.text_decoder, 'generate'):  # GPT/BioGPT models
                generation_args = generation_args or {}
                base_args = {
                    "input_ids": text_inputs,
                    "attention_mask": attention_mask,
                    "max_length": generation_args.get("max_length", 128),
                    "repetition_penalty": generation_args.get("repetition_penalty", 1.2),
                    "top_k": generation_args.get("top_k", 50),
                    "top_p": generation_args.get("top_p", 0.95),
                    "do_sample": True
                }

    # Only pass encoder_hidden_states if model supports cross-attention
                if hasattr(self.text_decoder, "config") and getattr(self.text_decoder.config, "add_cross_attention", False):
                    base_args["encoder_hidden_states"] = encoder_hidden_states

                generated_ids = self.text_decoder.generate(**base_args)
            else:  # scratch decoder
                generated_ids = None  # Scratch decoder does not implement `.generate()`

            return classification_output, generated_ids

        elif text_inputs is not None:
            seq_len = text_inputs.shape[1]
            repeated_features = projected_features.unsqueeze(1).repeat(1, seq_len, 1)

            if hasattr(self.text_decoder, 'forward'):
                out = self.text_decoder(
                    input_ids=text_inputs,
                    encoder_hidden_states=repeated_features
                )
                logits = out.logits if hasattr(out, "logits") else out  # support HuggingFace or scratch
            else:
                logits = self.text_decoder(text_inputs, encoder_hidden_states=repeated_features)

            return classification_output, logits

        return classification_output, None

    def freeze_encoder(self):
        for param in self.visual_backbone.parameters():
            param.requires_grad = False

    def unfreeze_encoder(self):
        for param in self.visual_backbone.parameters():
            param.requires_grad = True

# âœ… Helper for pipeline use
def build_model(encoder_name, decoder_name, vocab_size, pretrained_backbone=True, freeze_encoder=False):
    model = RadTexModel(
        encoder_name=encoder_name,
        decoder_name=decoder_name,
        vocab_size=vocab_size,
        pretrained_backbone=pretrained_backbone
    )
    if freeze_encoder:
        model.freeze_encoder()
    return model

-----
train.py
-----
# train.py
import torch
import torch.nn as nn
from transformers import AutoTokenizer
from torch.optim import Adam
from tqdm import tqdm
import pandas as pd
from pathlib import Path

from data import get_dataloader
from radtex_model import RadTexModel
def get_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")
    tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token
    tokenizer.padding_side = "left"
    return tokenizer

def train(model, train_loader, val_loader, tokenizer, epochs=1, lr=2e-5, save_path="radtex_model.pth", device="cuda", 
          only_classification=False, only_text_generation=False, generation_args=None):
    model = model.to(device)
    vocab_size = tokenizer.vocab_size

    classification_criterion = nn.CrossEntropyLoss()
    generation_criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-5)
    
    epoch_results = []
    phase = "classification" if only_classification else "text_generation"

    for epoch in range(epochs):
        

        print(f"\nğŸ”¥ Epoch {epoch+1}/{epochs}")
        model.train()
        total_class_loss, total_text_loss = 0.0, 0.0

        for images, reports, labels in tqdm(train_loader):
            images = images.to(device)
            labels = labels.to(device).long()

            tokenized = tokenizer(reports, padding=True, truncation=True, max_length=128, return_tensors="pt")
            text_inputs = tokenized["input_ids"].to(device)

            optimizer.zero_grad()

            # Forward pass
            class_output, text_output = model(
                images,
                text_inputs=text_inputs,
                generate=False,
                generation_args=generation_args or {}
            )

            total_loss = 0.0

            if not only_text_generation:
                class_loss = classification_criterion(class_output, labels)
                total_loss += class_loss
                total_class_loss += class_loss.item()

            if not only_classification and text_output is not None:
                text_loss = generation_criterion(text_output.view(-1, vocab_size), text_inputs.view(-1))
                total_loss += text_loss
                total_text_loss += text_loss.item()

            total_loss.backward()
            optimizer.step()

        print(f"ğŸ§ª Epoch [{epoch+1}/{epochs}] | Class Loss: {total_class_loss / len(train_loader):.4f}, Text Loss: {total_text_loss / len(train_loader):.4f}")

        validate(model, val_loader, tokenizer, vocab_size, classification_criterion, generation_criterion, device, only_classification, only_text_generation)
        epoch_results.append({
        "epoch": epoch + 1,
        "class_loss": total_class_loss / len(train_loader),
        "text_loss": total_text_loss / len(train_loader)
        })
    if save_path:
        save_dir = Path(save_path).parent
        (save_dir / "evaluations").mkdir(parents=True, exist_ok=True)
    if phase == "classification":
        filename = "classification_epoch_metrics.csv"
    else:
        filename = "text_gen_epoch_metrics.csv"
    pd.DataFrame(epoch_results).to_csv(save_dir / "evaluations" / filename, index=False)

    torch.save(model.state_dict(), save_path)
    print(f"âœ… Model saved at {save_path}")
    return model
def validate(model, val_loader, tokenizer, vocab_size, classification_criterion, generation_criterion, device, only_classification=False, only_text_generation=False):
    model.eval()
    correct, total = 0, 0
    total_class_loss, total_text_loss = 0.0, 0.0

    with torch.no_grad():
        for images, reports, labels in val_loader:
            images = images.to(device)
            labels = labels.to(device).long()

            tokenized = tokenizer(reports, padding=True, truncation=True, max_length=128, return_tensors="pt")
            text_inputs = tokenized["input_ids"].to(device)

            class_output, text_output = model(
                images,
                text_inputs=text_inputs,
                generate=False
            )

            if not only_text_generation:
                class_loss = classification_criterion(class_output, labels)
                total_class_loss += class_loss.item()

            if not only_classification and text_output is not None:
                text_loss = generation_criterion(text_output.view(-1, vocab_size), text_inputs.view(-1))
                total_text_loss += text_loss.item()

            predicted = class_output.argmax(dim=1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    acc = 100 * correct / total
    print(f"ğŸ” Validation â€” Class Loss: {total_class_loss / len(val_loader):.4f}, Text Loss: {total_text_loss / len(val_loader):.4f}, Accuracy: {acc:.2f}%")

-----
evaluate.py
-----
import torch
from sklearn.metrics import accuracy_score
import pandas as pd
import nltk
from pathlib import Path
from sklearn.metrics import precision_score, recall_score, f1_score
from data import get_dataloader
from transformers import AutoTokenizer
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from tqdm import tqdm
from radtex_model import build_model

def evaluate(model_path, batch_size, device, encoder, decoder,
             setup="local", csv_file="local/Final_AP_url_label_50000.csv",
             prompt="FINAL REPORT\n\n", num_datapoints=None,
             img_size=(224, 224), output_dir=None):

    tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    device = torch.device(device if torch.cuda.is_available() else "cpu")
    vocab_size = tokenizer.vocab_size

    model = build_model(encoder, decoder, vocab_size=vocab_size)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()

    test_loader = get_dataloader(
        mode="test",
        batch_size=batch_size,
        shuffle=False,
        setup=setup,
        csv_file=csv_file,
        num_samples=num_datapoints,
        img_size=img_size
    )

    # ğŸ‘‡ Read the CSV so we can access the full dataset with paths
    full_data = pd.read_csv(csv_file)

    prompt_inputs = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

    all_preds, all_labels = [], []
    all_generated_texts = []
    all_reference_texts = []

    rows = []

    print("\nğŸ“‹ Generating Reports from Images...\n")

    start_idx = 0  # to keep track of which samples we're evaluating

    with torch.no_grad():
        for images, reports, labels in tqdm(test_loader):
            batch_size_actual = images.size(0)

            images = images.to(device)
            labels = labels.to(device).long()

            class_output, generated_ids = model(
                images,
                text_inputs=prompt_inputs.repeat(images.size(0), 1),
                generate=True,
                generation_args={
                    "max_length": 128,
                    "repetition_penalty": 1.2,
                    "top_k": 50,
                    "top_p": 0.95
                }
            )

            predicted_labels = class_output.argmax(dim=1).cpu().numpy()
            generated_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]

            all_preds.extend(predicted_labels)
            all_labels.extend(labels.cpu().numpy())
            all_generated_texts.extend(generated_texts)
            all_reference_texts.extend(reports)

            # For each sample in batch, save results + path
            for i in range(batch_size_actual):
                true_label = labels[i].item()
                pred_label = predicted_labels[i]
                gen_text = generated_texts[i]
                ref_text = reports[i]

                # ğŸ“ Get image path from CSV
                img_path = full_data.iloc[start_idx + i]["local_urls"] if setup == "local" else full_data.iloc[start_idx + i]["hpc_urls"]

                rouge = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
                rouge_score = rouge.score(ref_text, gen_text)
                meteor = meteor_score([ref_text.split()], gen_text.split())

                rows.append({
                    "Image_Path": img_path,
                    "Predicted_Label": pred_label,
                    "True_Label": true_label,
                    "Generated_Text": gen_text,
                    "Reference_Text": ref_text,
                    "ROUGE1": rouge_score["rouge1"].fmeasure,
                    "ROUGE2": rouge_score["rouge2"].fmeasure,
                    "ROUGEL": rouge_score["rougeL"].fmeasure,
                    "METEOR": meteor
                })

            start_idx += batch_size_actual  # update position

    # Classification metric
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average="weighted", zero_division=0)
    recall = recall_score(all_labels, all_preds, average="weighted", zero_division=0)
    f1 = f1_score(all_labels, all_preds, average="weighted", zero_division=0)

    # Text metrics
    rouge = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
    rouge_scores = [rouge.score(ref, gen) for ref, gen in zip(all_reference_texts, all_generated_texts)]
    avg_rouge = {k: sum(d[k].fmeasure for d in rouge_scores) / len(rouge_scores) for k in rouge_scores[0]}

    meteor_scores = [
        meteor_score([ref.split()], gen.split()) for ref, gen in zip(all_reference_texts, all_generated_texts)
    ]
    avg_meteor = sum(meteor_scores) / len(meteor_scores)

    print("\nâœ… **Evaluation Results:**")
    print(f"ğŸ”¹ Accuracy: {accuracy:.4f}")
    print(f"ğŸ”¹ ROUGE: {avg_rouge}")
    print(f"ğŸ”¹ METEOR: {avg_meteor:.4f}")

    # ğŸ”¥ Save results if output_dir given
    if output_dir:
        output_dir = Path(output_dir)
        (output_dir / "evaluations").mkdir(parents=True, exist_ok=True)

        # Per-image
        pd.DataFrame(rows).to_csv(output_dir / "evaluations" / "per_image_results.csv", index=False)

        # Summary
        summary = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "rouge1": avg_rouge["rouge1"],
            "rouge2": avg_rouge["rouge2"],
            "rougeL": avg_rouge["rougeL"],
            "meteor": avg_meteor
        }

        pd.DataFrame([summary]).to_csv(output_dir / "evaluations" / "final_test_metrics_summary.csv", index=False)

        print(f"\nğŸ’¾ Saved evaluation results to: {output_dir / 'evaluations'}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, required=True)
    parser.add_argument("--encoder", type=str, default="densenet")
    parser.add_argument("--decoder", type=str, default="biogpt")
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--setup", type=str, choices=["local", "hpc"], default="local")
    parser.add_argument("--csv_file", type=str, default="local/Final_AP_url_label_50000.csv")
    parser.add_argument("--prompt", type=str, default="FINAL REPORT\n\n")
    parser.add_argument("--num_datapoints", type=int, default=None)
    parser.add_argument("--img_size", type=int, default=224)
    parser.add_argument("--output_dir", type=str, default=None)

    args = parser.parse_args()

    evaluate(
        model_path=args.model_path,
        batch_size=args.batch_size,
        device=args.device,
        encoder=args.encoder,
        decoder=args.decoder,
        setup=args.setup,
        csv_file=args.csv_file,
        num_datapoints=args.num_datapoints,
        img_size=(args.img_size, args.img_size),
        output_dir=args.output_dir
    )

-----
visualize.py
-----
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from PIL import Image
from pathlib import Path
import numpy as np

def format_text(gen_text, ref_text):
    gen_words = gen_text.split()
    ref_words = ref_text.split()

    formatted = []
    for word in gen_words:
        if word in ref_words:
            formatted.append(f"**{word}**")
        else:
            formatted.append(f"_\u0332{word}_")  # italic + underline

    return " ".join(formatted)

def plot_single_example(image_path, pred_label, true_label,
                        gen_text, ref_text, metrics_dict, output_path):

    img = Image.open(image_path).convert("L")

    fig, axs = plt.subplots(2, 2, figsize=(16, 12),
                            gridspec_kw={'width_ratios': [1, 1]})

    # Top-left: actual image
    axs[0, 0].imshow(img, cmap="gray")
    axs[0, 0].axis("off")
    axs[0, 0].set_title("Chest X-ray")

    # Top-right: labels + metrics
    axs[0, 1].axis("off")
    metrics_text = (f"Predicted Label: {pred_label}\n"
                    f"True Label: {true_label}\n")
    for key, value in metrics_dict.items():
        metrics_text += f"{key.upper()}: {value:.3f}\n"
    metrics_text += "\n**Bold** = correct words\n" \
                    "_Italic underlined_ = mismatch"
    axs[0, 1].text(0, 0.5, metrics_text, fontsize=12, wrap=True, va='center')

    # Bottom-left: generated report
    axs[1, 0].axis("off")
    axs[1, 0].set_title("Generated Report", loc="left")
    axs[1, 0].text(0, 0.5, format_text(gen_text, ref_text),
                   fontsize=10, wrap=True, va='center',
                   bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgrey', alpha=0.2))

    # Bottom-right: reference report
    axs[1, 1].axis("off")
    axs[1, 1].set_title("Reference Report", loc="left")
    axs[1, 1].text(0, 0.5, ref_text, fontsize=10, wrap=True, va='center',
                   bbox=dict(boxstyle="round,pad=0.5", facecolor='lightgrey', alpha=0.2))

    plt.tight_layout()
    plt.savefig(output_path, dpi=200)
    plt.close()

def line_plot_multi_y(data, output_path):
    fig, ax1 = plt.subplots(figsize=(10, 6))

    ax1.set_xlabel('Image Index')
    ax1.set_ylabel('ROUGE / METEOR', color='tab:blue')
    ax1.plot(data["ROUGEL"], 'o-', label="ROUGE-L", color='tab:blue')
    ax1.plot(data["ROUGE1"], 'o--', label="ROUGE-1", color='tab:cyan')
    ax1.plot(data["ROUGE2"], 'o--', label="ROUGE-2", color='tab:purple')
    ax1.plot(data["METEOR"], 's-', label="METEOR", color='tab:orange')
    ax1.tick_params(axis='y', labelcolor='tab:blue')

    ax2 = ax1.twinx()
    ax2.set_ylabel('Classification Accuracy', color='tab:green')
    acc = (data["Predicted_Label"] == data["True_Label"]).astype(int)
    ax2.plot(acc, 'x-', label="Accuracy", color='tab:green')
    ax2.tick_params(axis='y', labelcolor='tab:green')

    fig.legend(loc="upper right")
    plt.title("Metrics per Image")
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()

def plot_confusion(y_true, y_pred, output_path):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues", colorbar=False)
    plt.title("Confusion Matrix")
    plt.savefig(output_path)
    plt.close()

def plot_epoch_losses(run_dir, vis_dir):
    import pandas as pd
    eval_dir = Path(run_dir) / "evaluations"

    cls_file = eval_dir / "classification_epoch_metrics.csv"
    txt_file = eval_dir / "text_gen_epoch_metrics.csv"

    if not cls_file.exists() or not txt_file.exists():
        print("âš ï¸ Loss CSV files not found. Skipping epoch loss plot.")
        return

    cls = pd.read_csv(cls_file)
    txt = pd.read_csv(txt_file)

    txt["epoch"] += cls["epoch"].max()

    fig, ax1 = plt.subplots(figsize=(10, 6))

    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss', color='tab:red')
    ax1.plot(cls["epoch"], cls["class_loss"], label="Classification Loss", marker="o", color="tab:red")
    ax1.plot(txt["epoch"], txt["text_loss"], label="Text Generation Loss", marker="s", color="tab:orange")
    ax1.tick_params(axis='y', labelcolor='tab:red')

    # If accuracy is available, plot it on a second axis
    if "accuracy" in cls.columns:
        ax2 = ax1.twinx()
        ax2.set_ylabel('Accuracy', color='tab:green')
        ax2.plot(cls["epoch"], cls["accuracy"], label="Classification Accuracy", marker="x", color="tab:green")
        ax2.tick_params(axis='y', labelcolor='tab:green')

    fig.legend(loc="upper right")
    plt.title("Loss and Accuracy per Epoch")
    plt.tight_layout()
    plt.savefig(vis_dir / "metrics_per_epoch.png")
    plt.close()

def visualize_results(results_csv, output_dir):

    output_dir = Path(output_dir)
    vis_dir = output_dir / "visualizations"
    vis_dir.mkdir(parents=True, exist_ok=True)

    df = pd.read_csv(results_csv)

    # Sort best and worst by ROUGE-L
    best = df.sort_values("ROUGEL", ascending=False).head(10)
    worst = df.sort_values("ROUGEL", ascending=True).head(10)

    for idx, row in best.iterrows():
        outpath = vis_dir / f"best_{idx}.png"
        metrics_dict = {
            "rouge1": row["ROUGE1"],
            "rouge2": row["ROUGE2"],
            "rougel": row["ROUGEL"],
            "meteor": row["METEOR"]
        }
        plot_single_example(row["Image_Path"], row["Predicted_Label"], row["True_Label"],
                            row["Generated_Text"], row["Reference_Text"],
                            metrics_dict, outpath)

    for idx, row in worst.iterrows():
        outpath = vis_dir / f"worst_{idx}.png"
        metrics_dict = {
            "rouge1": row["ROUGE1"],
            "rouge2": row["ROUGE2"],
            "rougel": row["ROUGEL"],
            "meteor": row["METEOR"]
        }
        plot_single_example(row["Image_Path"], row["Predicted_Label"], row["True_Label"],
                            row["Generated_Text"], row["Reference_Text"],
                            metrics_dict, outpath)

    # Multi-metric line plot (multiple y-axis)
    line_plot_multi_y(df, vis_dir / "metrics_per_image.png")

    # Confusion matrix
    plot_confusion(df["True_Label"], df["Predicted_Label"], vis_dir / "confusion_matrix.png")

    # Loss + accuracy plot (epoch-wise)
    plot_epoch_losses(output_dir, vis_dir)

    print(f"âœ… Visualizations saved to: {vis_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--results_csv", type=str, required=True,
                        help="Path to per_image_results.csv from evaluate.py")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="Main output directory to save visualizations")

    args = parser.parse_args()

    visualize_results(args.results_csv, args.output_dir)

-----
run_pipeline.py
-----
# run_pipeline.py
import argparse
import os
import sys
import yaml
import shutil
import time
import torch
import platform
from pathlib import Path
from datetime import datetime
import nltk

from radtex_model import build_model
from data import get_dataloader
from train import train, get_tokenizer
from evaluate import evaluate
from visualize import visualize_results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="Chest-X-ray end-to-end pipeline (train â†’ eval â†’ visualise)")

# â€¢ bookkeeping
parser.add_argument("--name", type=str, default=None, help="Run folder name")
parser.add_argument("--save_path", type=str, default="bachelor_runs", help="Relative subfolder name under root")
parser.add_argument("--setup", type=str, choices=["local", "hpc"], default="local", help="Execution setup (affects paths)")
parser.add_argument("--config", type=Path, default=None, help="YAML config to load (overridden by later CLI flags)")

# â€¢ data
parser.add_argument("--csv_file", type=Path, default=Path("local/Final_AP_url_label_50000.csv"))
parser.add_argument("--batch_size", type=int, default=32)
parser.add_argument("--num_datapoints", type=int, default=None)
parser.add_argument("--img_size", type=int, default=224, help="Resize images to (img_size, img_size)")

# â€¢ model
parser.add_argument("--encoder", type=str, default="resnet", choices=["resnet", "densenet", "scratch_encoder"])
parser.add_argument("--decoder", type=str, default="gpt2", choices=["gpt2", "biogpt", "scratch_decoder"])
parser.add_argument("--freeze_encoder", action="store_true")

# â€¢ training phases
parser.add_argument("--training_phases", type=str, default="classification_then_text",
                    choices=["classification_only", "classification_then_text", "text_only"])
parser.add_argument("--epochs_classification", type=int, default=50)
parser.add_argument("--epochs_text_generation", type=int, default=50)
parser.add_argument("--learning_rate", type=float, default=2e-5)

# â€¢ generation hyper-params
parser.add_argument("--repetition_penalty", type=float, default=1.2)
parser.add_argument("--top_k", type=int, default=50)
parser.add_argument("--top_p", type=float, default=0.95)
parser.add_argument("--max_length", type=int, default=128)

# --------------- Parse args (with optional YAML) ---------------
args_cli = parser.parse_args()
if args_cli.config is not None:
    with open(args_cli.config, "r") as f:
        loaded_cfg = yaml.safe_load(f)
    parser.set_defaults(**loaded_cfg)
    args = parser.parse_args()
else:
    args = args_cli

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NLTK data path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if args.setup == "local":
    nltk.data.path.append("D:/Bachelor_project/local")
else:
    nltk.data.path.append("/work3/s224228/nltk_data")

start_time = time.time()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Resolve Save Path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
root_path = "D:/" if args.setup == "local" else "/work3/s224228/"
save_root = Path(root_path) / args.save_path
run_date = datetime.now().strftime("%d-%m-%Y_%H.%M")
run_name = f"{args.name}_{run_date}"
run_dir = (save_root / run_name).resolve()
run_dir.mkdir(parents=True, exist_ok=True)

# Save config
cfg = {k: (str(v) if isinstance(v, Path) else v) for k, v in vars(args).items()}
with open(run_dir / "config.yaml", "w") as f:
    yaml.safe_dump(cfg, f)
print(f"\nğŸ“ Outputs will be written to: {run_dir}\n")

# Log system info
with open(run_dir / "env.txt", "w") as f:
    print("Platform:", platform.platform(), file=f)
    print("Python:", sys.version.replace("\n", " "), file=f)
    print("CUDA available:", torch.cuda.is_available(), file=f)
    if torch.cuda.is_available():
        print("CUDA device:", torch.cuda.get_device_name(0), file=f)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Build tokenizer & model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = get_tokenizer()
model = build_model(
    encoder_name=args.encoder,
    decoder_name=args.decoder,
    vocab_size=tokenizer.vocab_size,
    pretrained_backbone=True,
    freeze_encoder=args.freeze_encoder
)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Loaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
loader_kwargs = {
    "batch_size": args.batch_size,
    "num_samples": args.num_datapoints,
    "setup": args.setup,
    "img_size": (args.img_size, args.img_size),
    "csv_file": args.csv_file
}
train_loader = get_dataloader(mode="train", **loader_kwargs)
val_loader = get_dataloader(mode="val", **loader_kwargs)
test_loader = get_dataloader(mode="test", **loader_kwargs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ckpt_path_cls = run_dir / "encoder_only.pth"
ckpt_path_full = run_dir / "full_model.pth"

if args.training_phases == "classification_only":
    print("\nğŸ”µ Phase: Classification Only Training")
    train(model, train_loader, val_loader, tokenizer,
          epochs=args.epochs_classification,
          lr=args.learning_rate,
          save_path=str(ckpt_path_cls),
          device=device,
          only_classification=True)
    final_ckpt = ckpt_path_cls

elif args.training_phases == "classification_then_text":
    print("\nğŸ”µ Phase 1: Classification Training")
    train(model, train_loader, val_loader, tokenizer,
          epochs=args.epochs_classification,
          lr=args.learning_rate,
          save_path=str(ckpt_path_cls),
          device=device,
          only_classification=True)
    print("\nğŸŸ  Phase 2: Text Generation Training")
    if args.freeze_encoder:
        model.freeze_encoder()
    train(model, train_loader, val_loader, tokenizer,
          epochs=args.epochs_text_generation,
          lr=args.learning_rate,
          save_path=str(ckpt_path_full),
          device=device,
          only_text_generation=True,
          generation_args={
              "repetition_penalty": args.repetition_penalty,
              "top_k": args.top_k,
              "top_p": args.top_p,
              "max_length": args.max_length
          })
    final_ckpt = ckpt_path_full

elif args.training_phases == "text_only":
    print("\nğŸŸ  Phase: Text Generation Only (Freeze Encoder)")
    model.freeze_encoder()
    train(model, train_loader, val_loader, tokenizer,
          epochs=args.epochs_text_generation,
          lr=args.learning_rate,
          save_path=str(ckpt_path_full),
          device=device,
          only_text_generation=True,
          generation_args={
              "repetition_penalty": args.repetition_penalty,
              "top_k": args.top_k,
              "top_p": args.top_p,
              "max_length": args.max_length
          })
    final_ckpt = ckpt_path_full
else:
    raise ValueError(f"Unknown training_phases: {args.training_phases}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
evaluate(
    model_path=str(final_ckpt),
    batch_size=args.batch_size,
    device=device,
    encoder=args.encoder,
    decoder=args.decoder,
    setup=args.setup,
    csv_file=str(args.csv_file),
    num_datapoints=args.num_datapoints,
    img_size=(args.img_size, args.img_size),
    output_dir=run_dir
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Visualisation (new version) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
results_csv = run_dir / "evaluations" / "per_image_results.csv"
visualize_results(str(results_csv), str(run_dir))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Runtime Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
total_time = time.time() - start_time
with open(run_dir / "runtime.txt", "w") as f:
    f.write(f"Total wall-clock time (s): {total_time:.2f}\n")
print(f"\nâœ… Done! Runtime: {total_time/3600:.2f} h â€” checkpoints & logs in {run_dir}")

-----